{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e62c77e-ed37-4671-b572-1123833f8b66",
   "metadata": {},
   "source": [
    "## Getting things ready\n",
    "\n",
    "By default this notebook will be running from the `notebooks` directory, let's install the root of this repository as a package to get the dependencies. Make sure that you're using a kernel that's based on a new virtual environment to prevent any dependency conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1f1566-323b-4dc1-bef1-a71058534b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install -c ../constraints.txt -e ..[dev,gcp,interactive]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b04baa",
   "metadata": {},
   "source": [
    "> In principle we don't need any of the optional packages to run the training code. We've included the `interactive` dependencies so that we can visualize data as part of the exploration. The `gcp` dependencies are for getting the data from BigQuery and **submitting** the training code to Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7240c9d-d99f-4c75-8cda-f256becf4dd9",
   "metadata": {},
   "source": [
    "Now we've installed all dependencies, we need to restart the kernel to be able to use those. **Please wait until the kernel is restarted** before you continue with the next steps; after the next cell is executed you'll be prompted with a dialog indicating that the _Kernel is restarting_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ab3d8-c581-4954-8f9f-fb8d1ca4f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d0e018-46f8-45a5-8c7f-d128348e91e3",
   "metadata": {},
   "source": [
    "We also would like to be able to use our module files without reinstalling it after every change, so let's add our package root to the `sys.path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d761f4-1f33-4627-91ab-b75323160599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b84ec6-c023-4ce9-be4c-a424185e14a7",
   "metadata": {},
   "source": [
    "One final step before we get started, we'd like our modules to be automatically reloaded after every change. This way if we make any changes to our scripts things will be available immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b2e28-0c3c-40f1-949b-d45d913d8c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45789210-4bb7-4863-83d8-227015886bcf",
   "metadata": {},
   "source": [
    "## Using Vertex AI for ad-hoc training\n",
    "\n",
    "Now we're ready to start with our first introduction to Vertex AI. The objective of this notebook is to illustrate how to use the capabilities of Vertex AI to train a custom model with no code changes.\n",
    "\n",
    "We've already prepared some simple code to train a simple _Random Forest Classifier_ using the popular _scikit-learn_ library. Please go ahead and check the source code to see that it's a very simple model and has no dependencies on Vertex AI at all. It returns an `accuracy` score when training is complete and stores the model file as well as some metrics in the provided output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59472333-14ef-4510-ab27-12885c983000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import task\n",
    "\n",
    "task.train(training_data_dir=\"../data\", validation_data_dir=None, output_dir=\"../outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15614039-9430-468e-851e-33b14c42d848",
   "metadata": {},
   "source": [
    "Well, running this locally is fine, but we're using local data and we're bound to the local hardware. Although we're running this on Vertex AI Workbench, where you also have access to more diverse hardware, we'll try to run our training script on Vertex AI where you have access to bigger, badder :) machines with many CPUs, large RAMs and even custom hardware such as GPUs and TPUs. For the sake of this experiment we'll stick to a simple machine, but you get the gist :)\n",
    "\n",
    "We've got a few more steps before we can submit our job.\n",
    "\n",
    "### Prereqs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c900a2a7-7ee6-4d05-b301-94f52cc29513",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"\n",
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list project --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2153bed2",
   "metadata": {},
   "source": [
    "For many services on GCP you need to define the region (location) where things should be executed. The `us-central1` (Iowa, US) is the default for most services, you can change it to another region, but then you need to make sure that you're consistent in your choice in other places as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b69e541-a2b3-4fad-9991-c808efe70de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\"\n",
    "    print(\"Region:\", REGION) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e28b6",
   "metadata": {},
   "source": [
    "We'll use the project id as the bucket name to make sure that we don't get any collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beccc6a5-b498-44cf-a302-781bc3e1532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://{PROJECT_ID}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4419df1c",
   "metadata": {},
   "source": [
    "The command below checks if the bucket already exists, if not creates a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ab623-b7bf-44d8-8f4e-4885eb93e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls -b $BUCKET_URI &> /dev/null || gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c6f344-d07a-4a91-9c58-225ee99845e9",
   "metadata": {},
   "source": [
    "### No data, no training\n",
    "\n",
    "Well, we'll also need some training data for our model. The sample code is based on the New York City Taxi fare data, also available in the BigQuery public dataset. We already have a sample our repository for testing purposes (`data/sample.csv`), but for the sake of completeness, we'll get a more fresh and larger snapshot for the Vertex AI training job.\n",
    "\n",
    "As our goal is to predict whether a taxi trip will yield a tip that's more than %20 of the fare, we'll do some pre-processing in BQ. The following snippet will load a sample of this data into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ef18a7-62da-47f7-8ec0-2366c4999644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.bigquery import Client, QueryJobConfig\n",
    "client = Client()\n",
    "\n",
    "query = \"\"\"SELECT\n",
    "        EXTRACT(MONTH from pickup_datetime) as trip_month,\n",
    "        EXTRACT(DAY from pickup_datetime) as trip_day,\n",
    "        EXTRACT(DAYOFWEEK from pickup_datetime) as trip_day_of_week,\n",
    "        EXTRACT(HOUR from pickup_datetime) as trip_hour,\n",
    "        TIMESTAMP_DIFF(dropoff_datetime, pickup_datetime, SECOND) as trip_duration,\n",
    "        trip_distance,\n",
    "        payment_type,\n",
    "        pickup_location_id as pickup_zone,\n",
    "        pickup_location_id as dropoff_zone,\n",
    "        IF((SAFE_DIVIDE(tip_amount, fare_amount) >= 0.2), 1, 0) AS tip_bin\n",
    "    FROM\n",
    "        `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2021` TABLESAMPLE SYSTEM (1 PERCENT)\n",
    "    LIMIT 10000\"\"\"\n",
    "job = client.query(query)\n",
    "df = job.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed84c36c-1f2b-4c27-b8e1-7ace05da4af0",
   "metadata": {},
   "source": [
    "Let's have a quick look at the data distribution, typically the exploration would be more comprehensive, but for the sake of this example we'll stick to something basic, looking at the trip duration (in seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dcb883-83ca-4140-ad80-b8d434e06dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.histplot(df.trip_duration);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037de1f1-65cb-4bfe-bab4-4952e20ee8e9",
   "metadata": {},
   "source": [
    "Hmm, there seems to be some outliers (trips of >10 hours), and 0 seconds, let's filter those out. The snippet below will keep only the trips that are between 5 minutes and 3 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84573011-2e1d-4ced-a720-e1f15d5509d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.trip_duration.between(300, 10800)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aa5608-5bf5-4b41-ac21-da8707887874",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df.trip_duration);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6152440-0f72-4c87-99b1-ec1f8bf8fdd0",
   "metadata": {},
   "source": [
    "Now we can put that sample data into GCS so that we can start running our training code (we're now directly writing to the GCS bucket from the pandas data frame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c19d67-40e7-4afe-8221-d2d748ac925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"{BUCKET_URI}/data/sample/sample.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a522117-76f1-4c58-992a-f283a98a7347",
   "metadata": {},
   "source": [
    "After putting the sample data into GCS, now it's time for the codebase. In order to run our codebase we first need to bundle it in the right format and put it on GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e8a4c9-2390-430b-a231-9b8447f17f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run this from the root of this repository\n",
    "!{sys.executable} -m build --sdist .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dec2fab",
   "metadata": {},
   "source": [
    "Let's copy the generated package on GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0032d1-a765-4614-8d15-ce4afa15af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -m cp ../dist/*.tar.gz $BUCKET_URI/code/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddc0e38-86b9-489e-be26-56c5cf1fcf7a",
   "metadata": {},
   "source": [
    "### Hooray, time for Vertex AI \n",
    "\n",
    "It's finally time to submit our job to Vertex AI. Since our model is a simple _scikit-learn_ model, we can use one of the pre-built containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cec56f-96d9-4959-8df2-11db6e7b342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkg_name = !ls -1t ../dist | head -n1\n",
    "print(f\"Latest package: {pkg_name[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77dfd09",
   "metadata": {},
   "source": [
    "We'll need to define the job first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7aedd-5952-4840-8f73-110b09b21f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=f\"{BUCKET_URI}/staging\")\n",
    "\n",
    "job = aiplatform.CustomPythonPackageTrainingJob(\n",
    "    display_name=\"my-first-training\",\n",
    "    python_package_gcs_uri=f\"{BUCKET_URI}/code/{pkg_name[0]}\",\n",
    "    python_module_name=\"trainer.task\",\n",
    "    container_uri=\"us-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-0:latest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a4562b",
   "metadata": {},
   "source": [
    "And then run it. This is a very simple model that's trained with a small dataset, so the job will take only a couple of minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6b207-b453-4567-8de5-0cd3eb679797",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.run(\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    args=[\n",
    "        \"--training-data-dir\", f\"{BUCKET_URI}/data/sample\",\n",
    "        \"--output-dir\", f\"{BUCKET_URI}/outputs\"\n",
    "    ]\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314d42d-1f4b-4f4f-9968-7a15015c1df2",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "It's fine that we can run a job manually and can use special hardware, but ideally we'd automate some of this for continuous training. So, next challenges are going to be around automation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "30040b708182827dd7270e3ff4373b1695e963a7e47229f570fd853418e89e0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
